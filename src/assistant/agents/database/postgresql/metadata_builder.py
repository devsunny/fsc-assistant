import json
import re
from collections import OrderedDict
from io import StringIO
from pathlib import Path
from typing import Any, Dict, List, Tuple, Union

from assistant.utils import split_fully_qualified_column_name

from ...llm.utils import create_llm_client
from ...postgresql.postgresql_client import PostgreSQLClient
from ...utils import json_utils
from .prompt_template import (
    COLUMN_DESCRIPTION_PROMPT,
    TABLE_DESCRIPTION_PROMPT2,
    USER_POSSIBLE_QUERY_PROMPT,
)


class MetadataBuilder:
    def __init__(self, dbclient: PostgreSQLClient):
        self.metadata = {}
        self.dbclient = dbclient
        self.llm_client = create_llm_client()
        self.logger = dbclient.logger
        self.extra_enhancement_columns = {}

    def add_all_tables(self, schema: str, table_name: str, *table_names: str):
        self.add_table(table_name, schema=schema)
        for tb in table_names:
            self.add_table(tb, schema=schema)

    def add_table(self, table_name, schema: str = None, columns: List[str] = None):
        if "." in table_name:
            schema, table_name = table_name.split(".")
        if schema is None:
            schema = "public"
        self.metadata[table_name] = {
            "columns": columns,
            "schema": schema,
            "table_name": table_name,
        }

    def add_enhance_columns(self, column_name, *more_columns):
        self._add_enhance_column(column_name)
        for col in more_columns:
            self._add_enhance_column(col)

    def _add_enhance_column(self, fully_qualified_column_name):
        schema_name, table_name, column_name = split_fully_qualified_column_name(
            fully_qualified_column_name
        )
        if schema_name is None and table_name in self.metadata:
            schema_name = self.metadata[table_name]["schema"]
        else:
            schema_name = "public"

        if table_name in self.extra_enhancement_columns:
            self.extra_enhancement_columns[table_name].append(
                (schema_name, table_name, column_name)
            )
        else:
            self.extra_enhancement_columns[table_name] = (
                schema_name,
                table_name,
                column_name,
            )

    def get_metadata(self):
        return self.metadata

    def get_markdown_smaple(
        self, schema: str, table_name: str, select_columns: List[str], limit: int = 10
    ) -> str:
        sample_data = self.get_sample_data(schema, table_name, select_columns, limit)
        return self.to_markdown(sample_data)

    def to_markdown(self, result: List[dict]) -> str:
        if not result:
            return "No data available."

        headers = result[0].keys()
        header_line = "| " + " | ".join(headers) + " |"
        separator_line = "| " + " | ".join(["---"] * len(headers)) + " |"

        rows = []
        for row in result:
            row_line = "| " + " | ".join(str(row[h]) for h in headers) + " |"
            rows.append(row_line)

        markdown_table = "\n".join([header_line, separator_line] + rows)
        return markdown_table

    def get_sample_data(
        self, schema: str, table_name: str, select_columns: List[str], limit: int = 10
    ) -> List[dict]:
        column_str = (
            ", ".join([f'"{col}"' for col in select_columns]) if select_columns else "*"
        )
        sample_query = f'SELECT {column_str}, RANDOM() random_order_key FROM "{schema}"."{table_name}" order by random_order_key LIMIT %s;'
        try:
            results = self.dbclient.execute_query(sample_query, (limit,))
            return results
        except Exception as e:
            self.logger.exception(
                f"Error fetching sample data for table '{table_name}'\n", exc_info=e
            )
            return []

    def build_table_description(self, table_schema: str, sample_data: str):
        try:
            json_text = ""
            for chunk in self.llm_client.invoke_model_generator(
                prompt=TABLE_DESCRIPTION_PROMPT2.format(
                    schema=table_schema,
                    sample_data=sample_data,
                )
            ):
                json_text += chunk
                print(chunk, end="", flush=True)
            print()
            try:
                import json

                clean_json_text = json_utils.extract_json_from_text(json_text)
                self.logger.debug(
                    "Table description generated by LLM.\n%s", clean_json_text
                )
                table_description = json.loads(clean_json_text)
                return table_description
            except Exception as e:
                self.logger.exception(
                    f"Error parsing JSON from LLM response\n", exc_info=e
                )
                self.logger.error(f"LLM response text:\n{json_text}")
        except Exception as e:
            self.logger.exception(f"LLM generation failed\n", exc_info=e)
        return None

    def build_table_metadata_base(
        self, schema: str, table_name: str, columns: List[str] = None
    ):
        query = f"""
        SELECT column_name, data_type
        FROM information_schema.columns
        WHERE  table_schema = %s and table_name = %s 
        ORDER BY ordinal_position;
        """
        column_filters = [col.lower() for col in columns] if columns else []
        schema_lines = [f"Schema: {schema}", f"Table: {table_name}", "Columns:"]
        table_base_metadata = []
        table_metadata = {}
        try:
            pks = self.dbclient.get_table_primary_key(f"{schema}.{table_name}")
            fks = self.dbclient.get_table_foreign_keys(f"{schema}.{table_name}")
            fks_map = {fk["column_name"]: fk for fk in fks}

            results = self.dbclient.execute_query(
                query,
                (
                    schema,
                    table_name,
                ),
            )
            if not results:
                self.logger.warning(
                    f"Table '{table_name}' does not exist or has no columns."
                )
                return None

            select_columns = []
            col_schemas = {}

            for row in results:
                column = row["column_name"]
                col_type = row["data_type"]
                if column_filters and column.lower() not in column_filters:
                    continue
                select_columns.append((column, col_type))
                key_tag = " "
                if row["column_name"] in pks:
                    key_tag += " [Primary Key]"
                if row["column_name"] in fks_map:
                    fk = fks_map[row["column_name"]]
                    key_tag += f" [Foreign Key -> {fk['foreign_table_name']}({fk['foreign_column_name']})]"

                col_schemas[column] = {
                    "data_type": col_type,
                    "key_tag": key_tag.strip(),
                }
                schema_lines.append(f" - {column}: {col_type}{key_tag}")

            sample_data = self.get_markdown_smaple(
                schema, table_name, [col for col, _ in select_columns], limit=10
            )

            schema_str = "\n".join(schema_lines)

            table_metadata["schema_name"] = schema
            table_metadata["table_name"] = table_name
            table_base_metadata.append("---")
            table_base_metadata.append(f"**Schema Name**: {schema}")
            table_base_metadata.append(f"**Table Name**: {table_name}")
            table_description = self.build_table_description(schema_str, sample_data)
            if table_description:
                if "overview" in table_description:
                    table_base_metadata.append(
                        f"**Table Overview**: {table_description['Overview']}"
                    )
                if "Observations" in table_description:
                    table_base_metadata.append(
                        f"**Table Observations**: {table_description['Observations']}"
                    )
                if "Example Use Case" in table_description:
                    table_base_metadata.append(
                        f"**Example Use Case**: {table_description['Example Use Case']}"
                    )
            table_base_metadata.append(f"**Columns**:")
            col_descriptions = (
                table_description.get("Columns", {}) if table_description else {}
            )
            col_metadatas = []
            for column_name, col_type in select_columns:
                col_desc = col_descriptions.get(column_name, "")
                key_tag = col_schemas[column_name]["key_tag"]
                desc_tag = f" - {col_desc}" if col_desc else ""
                table_base_metadata.append(
                    f" - **{column_name}**: {col_type}{key_tag}{desc_tag}"
                )
                col_metadatas.append(
                    {
                        "column_name": column_name,
                        "column_type": col_type,
                        "key_tag": key_tag,
                        "description": col_desc,
                    }
                )

            table_metadata["columns"] = col_metadatas

        except Exception as e:
            self.logger.exception(f"LLM enhancement failed\n", exc_info=e)
            self.logger.error(f"Error enhancing schema for table '{table_name}': {e}")

        base_metadata = "\n".join(table_base_metadata)
        table_metadata["markdown"] = base_metadata
        return table_metadata

    def create_most_frequent_query(
        self, metadata_bases: List[Dict[str, Any]], top_n: int = 5
    ) -> str:
        database_description = "\n".join(
            [meta["markdown"] for meta in metadata_bases.values() if meta]
        )
        try:
            json_text = ""
            for chunk in self.llm_client.invoke_model_generator(
                prompt=USER_POSSIBLE_QUERY_PROMPT.format(
                    database_description=database_description,
                    top_n=top_n,
                )
            ):
                json_text += chunk
                print(chunk, end="", flush=True)
            print()
            try:
                clean_json_text = json_utils.extract_json_from_text(json_text)
                self.logger.debug(
                    "User possible queries generated by LLM.\n%s", clean_json_text
                )
                user_queries = json.loads(clean_json_text)["user_queries"]
                return user_queries
            except Exception as e:
                self.logger.exception(
                    f"Error parsing JSON from LLM response\n", exc_info=e
                )
                self.logger.error(f"LLM response text:\n{json_text}")
        except Exception as e:
            self.logger.exception(f"LLM generation failed\n", exc_info=e)
        return None

    def build_metadata_base(self):
        table_metadatas = OrderedDict()
        for table_name in self.metadata.keys():
            tbl_md = self.build_table_metadata_base(
                schema=self.metadata[table_name]["schema"],
                table_name=self.metadata[table_name]["table_name"],
                columns=self.metadata[table_name]["columns"],
            )
            table_metadatas[table_name] = tbl_md
        return table_metadatas

    def split_column_infos(
        self, column_infos: List[List[Tuple[str, str]]], limit: int = 15
    ) -> List[List[Tuple[str, str]]]:
        """
        Split column info list into chunks of specified size.

        Args:
            column_infos: List of column info tuples (column_name, data_type)
            limit: Maximum number of columns per chunk

        Returns:
            List of column info chunks
        """
        chunks = []
        for i in range(0, len(column_infos), limit):
            chunks.append(column_infos[i : i + limit])
        return chunks

    def enrich_columns_metadata(self):
        if not self.extra_enhancement_columns:
            return {}
        enriched_metdata = {}
        for table_name in self.extra_enhancement_columns:
            cols = self.extra_enhancement_columns[table_name]
            if not cols:
                continue
            col_names = [col_name for _, _, col_name in cols]
            schema_name, table_name, _ = cols[0]
            col_enriched_metdata = self.enhance_column_metadata(
                schema_name, table_name, col_names
            )

        enriched_metdata[table_name] = col_enriched_metdata
        return enriched_metdata

    def enhance_column_metadata(
        self, schema_name, table_name, select_columns: List[str]
    ):
        if not select_columns:
            return []
        assert re.search(
            r"^[a-zA-Z_][a-zA-Z0-9_]*$", table_name
        ), f"Invalid table name: {table_name}"
        assert re.search(
            r"^[a-zA-Z_][a-zA-Z0-9_]*$", schema_name
        ), f"Invalid schema name: {schema_name}"
        descriptions = []
        try:
            for column_chunks in self.split_column_infos(select_columns, limit=20):
                columns_info = []
                for column in column_chunks:
                    data_query = f"""select "{column}" from (
                        select "{column}", random() as rand from (
                        select distinct "{column}" from "{schema_name}"."{table_name}" where "{column}" is not null
                        ) c order by rand limit 100
                        ) r"""
                    results = self.execute_query(data_query)
                    try:
                        sample_values = json.dumps(
                            [row[column] for row in results],
                            cls=json_utils.CustomJsonEncoder,
                        )
                    except Exception as e:
                        sample_values = str([row[column] for row in results])

                    columns_info.append(
                        f"- Column Name: {column}\n - Sample Values: {sample_values}\n"
                    )

                columns_info_str = "\n".join(columns_info)

                prompt = COLUMN_DESCRIPTION_PROMPT.format(columns_info=columns_info_str)
                response_text = ""
                for chunk in self.llm_client.invoke_model_generator(prompt=prompt):
                    response_text += chunk
                    print(chunk, end="", flush=True)
                response_text = response_text.strip().split("</think>")[-1].strip()
                started = False
                clean_text = ""
                for line in StringIO(response_text):
                    if started is False and line.strip().startswith("```"):
                        started = True
                    if started and line.strip().endswith("```"):
                        started = False
                        clean_text += "\n"
                    elif started is True:
                        clean_text += line

                descriptions.append(clean_text)

        except Exception as e:
            self.logger.error(
                f"Error retrieving column descriptions for table '{table_name}': {e}"
            )
            raise
        return descriptions

    def build_markdown_metadata(self, metadata_file_path: Union[Path, str] = None):
        markdown_output = ["**Database Schema**\n"]
        metadata_bases = self.build_metadata_base()
        user_queries = self.create_most_frequent_query(metadata_bases, top_n=10)
        enriched_col_descs = {}
        if self.extra_enhancement_columns:
            enriched_col_descs = self.enrich_columns_metadata()

        for table_name in metadata_bases:
            table_metadata = metadata_bases.get(table_name)
            if table_metadata is None:
                self.logger.error("no metadata for table: [%s]", table_name)
                continue

            markdown_output.append(table_metadata["markdown"])
            if table_name in enriched_col_descs:
                markdown_output.extends(enriched_col_descs[table_name])

        markdown_output = [meta["markdown"] for meta in metadata_bases.values() if meta]

        markdown_output.append(f"\n **The most used queries**\n")
        for query in user_queries:
            markdown_output.append(f" - {query["user"]}")
            markdown_output.append(f"```sql\n{query["sql"]}\n```")

        markdown_text = "\n".join(markdown_output)
        if metadata_file_path is not None:
            if isinstance(metadata_file_path, str):
                metadata_file_path = Path(metadata_file_path)
            metadata_file_path.parent.mkdir(exist_ok=True)
            metadata_file_path.write_text(markdown_text, encoding="utf-8")
        return markdown_text


if __name__ == "__main__":
    from ...postgresql.postgresql_client import PostgreSQLClient

    dbclient = PostgreSQLClient.from_env()
    md_builder = MetadataBuilder(dbclient)
    md_builder.add_table("meta_dev.users_ldap")
    md_builder.add_table("meta_dev.roles")
    md_builder.add_table("meta_dev.user_role")
    meta_database = md_builder.build_markdown_metadata()
    print(
        "---------------------------------------------------------------------------------"
    )
    print(meta_database)
